


@inproceedings{kaddourNoTrainNo2023a,
	title = {No {Train} {No} {Gain}: {Revisiting} {Efficient} {Training} {Algorithms} {For} {Transformer}-based {Language} {Models}},
	shorttitle = {No {Train} {No} {Gain}},
	url = {http://arxiv.org/abs/2307.06440},
	doi = {10.48550/arXiv.2307.06440},
	abstract = {The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.},
	urldate = {2023-09-06},
	author = {Kaddour, Jean and Key, Oscar and Nawrot, Piotr and Minervini, Pasquale and Kusner, Matt J.},
	month = jul,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance},
  pdf={https://arxiv.org/pdf/2307.06440.pdf},
  selected={true},
  code={https://github.com/jeankaddour/notrainnogain},
  booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
}



@inproceedings{wangEvaluatingSelfSupervisedLearning2023,
	title = {Evaluating {Self}-{Supervised} {Learning} for {Molecular} {Graph} {Embeddings}},
	url = {http://arxiv.org/abs/2206.08005},
	abstract = {Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present 'Molecular Graph Representation Evaluation' (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inconsistencies between inferences drawn solely from existing datasets and those derived from more nuanced probing. These findings suggest that current evaluation methodologies fail to capture the entirety of the landscape.},
	urldate = {2023-09-22},
	author = {Wang, Hanchen and Kaddour, Jean and Liu, Shengchao and Tang, Jian and Lasenby, Joan and Liu, Qi},
	month = jul,
  code={https://github.com/hansen7/MolGraphEval},
	year = {2023},
  booktitle={Advances in Neural Information Processing Systems},
    selected={true},

}



@misc{kaddourChallengesApplicationsLarge2023,
	title = {Challenges and {Applications} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.10169},
	doi = {10.48550/arXiv.2307.10169},
	abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10169 [cs]},
  pdf={https://arxiv.org/pdf/2307.10169.pdf},
  selected={true},
}



@article{lynch2023spawrious,
  title={Spawrious: A benchmark for fine control of spurious correlation biases},
  author={Lynch, Aengus and Dovonon, Gb{\`e}tondji JS and Kaddour, Jean and Silva, Ricardo},
  journal={arXiv preprint arXiv:2303.05470},
  year={2023},
  selected={true},
  website={https://aengusl.github.io/spawrious.github.io/},
  pdf={https://arxiv.org/pdf/2303.05470.pdf}

}


@misc{kaddourMiniPileChallengeDataEfficient2023,
	title = {The {MiniPile} {Challenge} for {Data}-{Efficient} {Language} {Models}},
	pdf = {http://arxiv.org/abs/2304.08442},
	doi = {10.48550/arXiv.2304.08442},
	abstract = {The ever-growing diversity of pre-training text corpora has equipped language models with generalization capabilities across various downstream tasks. However, such diverse datasets are often too large for academic budgets; hence, most research on Transformer architectures, training procedures, optimizers, etc. gets conducted on smaller, homogeneous datasets. To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents. MiniPile is a 6GB subset of the deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using \$k\$-means, and (3) filter out low-quality clusters. To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only \$1.9{\textbackslash}\%\$/\$2.5{\textbackslash}\%\$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on \$2.6\$x/\$745\$x the amount of data. MiniPile is available at https://huggingface.co/datasets/JeanKaddour/minipile.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Kaddour, Jean},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08442 [cs]},
  selected={true},
  website={https://huggingface.co/datasets/JeanKaddour/minipile}
}

@inproceedings{
kaddour2022stop,
title={Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging},
author={Jean Kaddour},
booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
year={2022},
url={https://openreview.net/forum?id=0OrABUHZuz},
selected={true},
pdf={https://openreview.net/pdf?id=0OrABUHZuz},
code={https://github.com/JeanKaddour/LAWA}
}

@misc{kaddour2022causal,
    title={Causal Machine Learning: A Survey and Open Problems},
    author={Jean Kaddour and Aengus Lynch and Qi Liu and Matt J. Kusner and Ricardo Silva},
    year={2022},
    eprint={2206.15475},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    selected={true},
    pdf={https://arxiv.org/pdf/2206.15475.pdf},
    website={https://www.causal-machine-learning.com/}
}



@inproceedings{
kaddour2022when,
title={When Do Flat Minima Optimizers Work?},
author={Jean Kaddour and Linqing Liu and Ricardo Silva and Matt Kusner},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=vDeh2yxTvuh},
selected={true},
pdf={https://openreview.net/pdf?id=vDeh2yxTvuh}
}

@inproceedings{
zantedeschi2023dag,
title={{DAG} Learning on the Permutahedron},
author={Valentina Zantedeschi and Luca Franceschi and Jean Kaddour and Matt Kusner and Vlad Niculae},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=m9LCdYgN8-6},
selected={true},
pdf={https://openreview.net/pdf?id=m9LCdYgN8-6}
}

@inproceedings{kaddour2021graph,
      title={Causal Effect Inference for Structured Treatments},
      author={Jean Kaddour and Yuchen Zhu and Qi Liu and Matt J. Kusner and Ricardo Silva},
      booktitle = {NeurIPS},
      year={2021},
      selected={true},
      pdf = {https://arxiv.org/pdf/2106.01939.pdf},
      code={https://github.com/JeanKaddour/GIN},
      video={https://www.youtube.com/watch?v=lnVDe1TbkbQ},
      publisher = {Curran Associates, Inc.},
      volume = {35},
      editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
      poster={sin_poster.pdf}
}

@inproceedings{paml,
  author={Kaddour, Jean and Sæmundsson, Steindór and Deisenroth, Marc},
  booktitle = {NeurIPS},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages = {20813--20822},
  publisher = {Curran Associates, Inc.},
  title = {Probabilistic Active Meta-Learning},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf},
  volume = {34},
  year = {2020},
  selected={true},
  code= {https://github.com/JeanKaddour/PAML},
  video={https://www.youtube.com/watch?v=ipN-bK6Od3U},
  poster={paml_poster.pdf}
}
